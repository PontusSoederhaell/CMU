{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn-g4uFqlPfW"
      },
      "source": [
        "# Recitation 0L : Dataloaders\n",
        "\n",
        "The purpose of this notebook is to understand the main concepts behind the PyTorch DataLoader and see a pedogogical, trivial example of using DataLoaders. Additionally, DataLoader customization (the collate function) and GPU memory efficiency will be discussed.\n",
        "\n",
        "PyTorch Reference: https://pytorch.org/docs/stable/data.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contents\n",
        "\n",
        "1. Introduction to PyTorch DataLoader\n",
        "2. Initializing a DataLoader Object\n",
        "3. On-the-Fly Data Loading\n",
        "4. Handling Different Batching Strategies\n",
        "5. Creating a Custom Batch Sampler\n",
        "6. Customizing Data Loading with the Collate Function\n",
        "7. Leveraging Multi-Process Data Loading and Pin Memory"
      ],
      "metadata": {
        "id": "ECjP-pAKoa3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch DataLoader"
      ],
      "metadata": {
        "id": "bR1JrZ45rZ1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Zf31rVaWrRHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRyHXJLC2HdQ"
      },
      "source": [
        "## Manual data feed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kUBWT2kA-Tm"
      },
      "source": [
        "**1 epoch**: one complete pass of the training dataset through the algorithm\n",
        "\n",
        "**batch_size**: the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you will need.\n",
        "\n",
        "\n",
        "**No of iterations = No of batches**: number of passes, each pass using batch_size number of examples.\n",
        "\n",
        "Example: With 100 training examples and batch size of 20 it will take 5 iterations to complete 1 epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "x = a list of 10000 input samples\n",
        "y = a list of 10000 target labels corresponding to x\n",
        "\n",
        "# Load data manually in batches\n",
        "for epoch in range(10):\n",
        "    for i in range(n_batches):\n",
        "        # Local batches and labels\n",
        "        local_X, local_y = x[i*n_batches:(i+1)*n_batches,], y[i*n_batches:(i+1)*n_batches,]\n",
        "\n",
        "        # Your model\n",
        "        [...]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4MVhNRpVScZO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFeJx8YzA-Tm"
      },
      "source": [
        "# Dataloaders (PyTorch)\n",
        "\n",
        "Documentation:\n",
        "[Read Docs](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "\n",
        "The Dataset retrieves our dataset's features and labels one sample at a time. While training a model, we typically want to\n",
        "\n",
        "1.   Pass samples in “minibatches”\n",
        "2.   Reshuffle the data at every epoch to reduce model overfitting\n",
        "3.   Use Python's multiprocessing to speed up data retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RHffau3A-To"
      },
      "source": [
        "# 1- Sample DataLoader\n",
        "\n",
        "Handles data loading logic\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "# Dataloader will use dataset to create batches, process data etc.\n",
        "# Visit Dataset Recitation for more details\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    # constructor, in this case it contains the data\n",
        "    def __init__(self, xs, ys):\n",
        "        self.input = input\n",
        "        self.target = target\n",
        "\n",
        "    # returns the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    # returns the item at index i\n",
        "    def __getitem__(self, i):\n",
        "        return self.input[i], self.target[i]"
      ],
      "metadata": {
        "id": "ENf4LWb-MZ5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to train a model to learn that the target = 2 x input, and hence created the following dataset:"
      ],
      "metadata": {
        "id": "uD7Lw07mEbft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are creating a dummy dataset to test Dataloaders\n",
        "input = list(range(10))\n",
        "target = list(range(0, 20, 2))\n",
        "print('input values: ', input)\n",
        "print('target values: ', target)\n",
        "\n",
        "# Create an instance of MyDataset class\n",
        "dataset = MyDataset(input, target)\n",
        "print(\"The second sample is: \", dataset[2]) # returns the tuple (input[2], target[2])\n",
        "# This is basically same as\n",
        "print(\"The second sample is: \", dataset.__getitem__(2))\n",
        "# Which the dataloader needs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6rOUn8rM53n",
        "outputId": "9f0e3eee-7612-466b-cc26-0cc81423a1d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input values:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "target values:  [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "The second sample is:  (2, 4)\n",
            "The second sample is:  (2, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Initializing a DataLoader Object\n",
        "### Let's look at different ways of creating the Dataloader object using the Dataloader class\n"
      ],
      "metadata": {
        "id": "J1fO482wRkAb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q_QDR9FA-To",
        "outputId": "baa843f7-54d2-4fa4-e8aa-28f763fd9995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of inputs: tensor([0]), batch of labels: tensor([0])\n",
            "batch of inputs: tensor([1]), batch of labels: tensor([2])\n",
            "batch of inputs: tensor([2]), batch of labels: tensor([4])\n",
            "batch of inputs: tensor([3]), batch of labels: tensor([6])\n",
            "batch of inputs: tensor([4]), batch of labels: tensor([8])\n",
            "batch of inputs: tensor([5]), batch of labels: tensor([10])\n",
            "batch of inputs: tensor([6]), batch of labels: tensor([12])\n",
            "batch of inputs: tensor([7]), batch of labels: tensor([14])\n",
            "batch of inputs: tensor([8]), batch of labels: tensor([16])\n",
            "batch of inputs: tensor([9]), batch of labels: tensor([18])\n"
          ]
        }
      ],
      "source": [
        "# batch size of 1, so we the size of x and y is 1 and no shuffling\n",
        "for x, y in DataLoader(dataset):\n",
        "    print(f\"batch of inputs: {x}, batch of labels: {y}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- On-the-Fly Data Loading\n",
        "\n",
        "The key to handling large datasets is to load and process data in batches, only when needed. This strategy, known as on-the-fly or lazy loading, can be implemented in PyTorch by customizing the ```Dataset``` class."
      ],
      "metadata": {
        "id": "bAtOaB3tjjkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LargeImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = os.listdir(image_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "# Usage\n",
        "image_dir = '/path/to/large/image/dataset'\n",
        "\n",
        "# on-the-fly augmantation\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((256, 256)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "large_dataset = LargeImageDataset(image_dir, transform=transform)\n"
      ],
      "metadata": {
        "id": "MspFmp8jjtqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Handling Different Batching Strategies"
      ],
      "metadata": {
        "id": "mCvUEsjPgCWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size of 4, so x and y both have a size of 4, no shuffling\n",
        "for x, y in DataLoader(dataset, batch_size=4):\n",
        "    print(f\"batch of inputs: {x}, batch of labels: {y}\")"
      ],
      "metadata": {
        "id": "klMIFFxPR7qY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27605852-570f-4744-b132-b5b523bf90f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of inputs: tensor([0, 1, 2, 3]), batch of labels: tensor([0, 2, 4, 6])\n",
            "batch of inputs: tensor([4, 5, 6, 7]), batch of labels: tensor([ 8, 10, 12, 14])\n",
            "batch of inputs: tensor([8, 9]), batch of labels: tensor([16, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHzO_2gLA-To",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca79a91-fc07-4e0e-e0e4-b1f469c443d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of inputs: tensor([3, 5, 8, 2]), batch of labels: tensor([ 6, 10, 16,  4])\n",
            "batch of inputs: tensor([1, 9, 7, 6]), batch of labels: tensor([ 2, 18, 14, 12])\n",
            "batch of inputs: tensor([4, 0]), batch of labels: tensor([8, 0])\n"
          ]
        }
      ],
      "source": [
        "# batch size of 4, so x and y both have a size of 4, random shuffle\n",
        "for x, y in DataLoader(dataset, batch_size=4, shuffle=True):\n",
        "    print(f\"batch of inputs: {x}, batch of labels: {y}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size of 4, drop the last batch with less than 4 samples\n",
        "for x, y in DataLoader(dataset, batch_size=4, shuffle=True, drop_last=True):\n",
        "    print(f\"batch of inputs: {x}, batch of labels: {y}\")"
      ],
      "metadata": {
        "id": "PUsk2wl9QZ0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4540d658-f723-4f55-e04b-6a4464b89e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch of inputs: tensor([2, 1, 3, 5]), batch of labels: tensor([ 4,  2,  6, 10])\n",
            "batch of inputs: tensor([8, 4, 0, 6]), batch of labels: tensor([16,  8,  0, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 5- Creating a Custom [Batch Sampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.Sampler)\n",
        "\n",
        "In PyTorch, a Batch Sampler is used to define how batches are formed from the dataset. The default behavior is to sequentially or randomly sample elements to form a batch. However, there are scenarios where you may need more control over how batches are created, such as when dealing with sequences of varying lengths or when implementing certain types of sampling strategies like stratified sampling for imbalanced data. In such cases, a custom batch sampler is invaluable.\n",
        "\n",
        "\n",
        "A custom batch sampler in PyTorch is a class that implements two key methods: ```__iter__ ``` and ```__len__```. The ```__iter__``` method yields a list of indices representing a single batch, and ```__len__``` returns the number of batches expected in an epoch.\n",
        "\n"
      ],
      "metadata": {
        "id": "pZJyZmLzWvLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an object of the custom dataset class, here we will create a toy dataset that contains odd and even numbers along with their labels\n",
        "class EvenOddNumberDataset(Dataset):\n",
        "    def __init__(self, start, end):\n",
        "\n",
        "        self.numbers = list(range(start, end))\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # Return the total number of items in the dataset.\n",
        "\n",
        "        return len(self.numbers)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Retrieve an item by its index.\n",
        "\n",
        "        number = self.numbers[idx]\n",
        "        label = 1 if number % 2 != 0 else 0  # 1 for odd, 0 for even\n",
        "        return number, label\n",
        "\n",
        "# Usage example\n",
        "start = 0\n",
        "end = 100\n",
        "dataset = EvenOddNumberDataset(start, end)\n",
        "\n",
        "# Example of accessing the dataset\n",
        "for i in range(5):\n",
        "    print(dataset[i])\n"
      ],
      "metadata": {
        "id": "ddbX56XTa3RT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c6ebda-5653-4e6b-ce69-b66f2e4d4b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0)\n",
            "(1, 1)\n",
            "(2, 0)\n",
            "(3, 1)\n",
            "(4, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyBatchSampler(Sampler):\n",
        "    def __init__(self, dataset, batch_size):\n",
        "\n",
        "        # Initialize the batch sampler.\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = [label for _, label in dataset]\n",
        "        self.class_indices = self._get_class_indices()\n",
        "        self.num_batches = np.ceil(len(self.dataset) / batch_size)\n",
        "\n",
        "    def _get_class_indices(self):\n",
        "\n",
        "        # Group dataset indices by class (even/odd).\n",
        "\n",
        "        class_indices = {}\n",
        "        for idx, label in enumerate(self.labels):\n",
        "            if label not in class_indices:\n",
        "                class_indices[label] = []\n",
        "            class_indices[label].append(idx)\n",
        "        return class_indices\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        # Iterator method to yield batches.\n",
        "\n",
        "        for _ in range(int(self.num_batches)):\n",
        "            batch = []\n",
        "            for class_idx in self.class_indices.values():\n",
        "                samples_per_class = int(self.batch_size / len(self.class_indices))\n",
        "                selected_indices = np.random.choice(class_idx, samples_per_class, replace=False)\n",
        "                batch.extend(selected_indices)\n",
        "            np.random.shuffle(batch)\n",
        "            yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # Return the total number of batches.\n",
        "\n",
        "        return int(self.num_batches)\n"
      ],
      "metadata": {
        "id": "ooviXHqxY10f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sampler = MyBatchSampler(dataset, batch_size=16)\n",
        "data_loader = DataLoader(MyDataset, batch_sampler=batch_sampler)"
      ],
      "metadata": {
        "id": "ArH0H2-rY-q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJUw-SARA-Tp"
      },
      "source": [
        "\n",
        "# 6- Collate function\n",
        "\n",
        "A dataloader parameter which can be customized to achieve custom automatic batching.\n",
        "\n",
        "You may apply some transformation in the collate function;\n",
        "One can choose to apply transformation in the collate function instaed of dataset class if transformation needs to be applied on batches.\n",
        "Also, since data loader support multiprocess through multi-workers, hence ```collate_fn()``` also can take advantage of multi-workers performance speed up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqd2U3DcA-Tp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5ae275-e30f-4d2b-9f6b-12eb92033cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input values:  [0 1 2 3 4 5 6 7 8 9]\n",
            "target values:  [ 0  2  4  6  8 10 12 14 16 18]\n",
            "batch of inputs: [ 1.53281263 -0.90575292  0.48771311 -1.25411943  0.1393466 ], batch of labels: (16, 2, 10, 0, 8)\n",
            "batch of inputs: [ 0.62092042  0.23284516  1.39707095 -0.93138063 -1.31945589], batch of labels: (14, 12, 18, 6, 4)\n"
          ]
        }
      ],
      "source": [
        "# Create an object of the custom dataset class\n",
        "class MyNormalDataset(Dataset):\n",
        "    # constructor, in this case it contains the data\n",
        "    def __init__(self, xs, ys):\n",
        "        self.input = input\n",
        "        self.target = target\n",
        "\n",
        "    # returns the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    # returns the item at index i\n",
        "    def __getitem__(self, i):\n",
        "        return self.input[i], self.target[i]# create a dict of arguments, another way of passing arguments\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        x, y = zip(*batch)\n",
        "        x_mean = np.mean(x)\n",
        "        x_std = np.std(x)\n",
        "        x_normal = (x-x_mean)/(x_std+1e-9)\n",
        "        return x_normal, y\n",
        "\n",
        "\n",
        "input = np.array(list(range(10)))\n",
        "target = np.array(list(range(0, 20, 2)))\n",
        "print('input values: ', input)\n",
        "print('target values: ', target)\n",
        "\n",
        "# Create an instance of MyDataset class\n",
        "dataset = MyNormalDataset(input, target)\n",
        "# Use the custom collate_fn\n",
        "# pass the arguments\n",
        "train_dataloader_custom = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn= dataset.collate_fn)\n",
        "\n",
        "# Display collated inputs and labels.\n",
        "for i, (x, y) in enumerate(train_dataloader_custom):\n",
        "    print(f\"batch of inputs: {x}, batch of labels: {y}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7- Single and multi-process loading"
      ],
      "metadata": {
        "id": "_miJSuhbX4rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the ```num_workers``` to specify how many subprocesses to use for data loading. \\\n",
        "0 means that the data will be loaded in the main process. \\"
      ],
      "metadata": {
        "id": "Y04JQml3YPJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 subprocesses\n",
        "train_dataloader_fast = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn= dataset.collate_fn, num_workers=2)"
      ],
      "metadata": {
        "id": "Kg8Wc1HSX8cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The maximum subprocesses you can use depends on the machine you are training on\n",
        "# you can try to increase it until you see a warning.\n",
        "\n",
        "train_dataloader_fast = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn= dataset.collate_fn, num_workers=4)"
      ],
      "metadata": {
        "id": "nEvm1fHvaGC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16edeb5d-f329-4af1-fde0-f9da0c032621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use ```pin_memory``` to copy Tensors into device/CUDA pinned memory before returning them -> faster processing."
      ],
      "metadata": {
        "id": "--nS-mwFYnUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_faster = DataLoader(dataset, batch_size=5, shuffle=True, collate_fn= dataset.collate_fn, num_workers=4, pin_memory= True)"
      ],
      "metadata": {
        "id": "EHIMnMhuYnkC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}